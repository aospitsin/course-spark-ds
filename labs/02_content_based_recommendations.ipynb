{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>artem.spitsin_lab02</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3b7f8e3390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/anaconda/envs/bd9/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]     = \"/usr/hdp/current/spark2-client\"\n",
    "\n",
    "spark_home = os.environ.get(\"SPARK_HOME\", None)\n",
    "if not spark_home:\n",
    "    raise ValueError(\"SPARK_HOME environment variable is not set\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))\n",
    "\n",
    "import pyspark\n",
    "from pyspark import keyword_only\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Transformer \n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd # For using pd.notna\n",
    "\n",
    "conf = SparkConf()\\\n",
    "       .setAppName(\"artem.spitsin_lab02\")\\\n",
    "       .set(\"spark.executor.instances\", \"2\")\n",
    "\n",
    "ss = SparkSession\\\n",
    "     .builder\\\n",
    "     .appName(\"artem.spitsin_lab02\")\\\n",
    "     .config(conf=conf)\\\n",
    "     .getOrCreate()\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a custom RegexTokenizer to add unicode support\n",
    "class CustomRegexTokenizer(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    Tokenizes a string based on a substring search based on a given \n",
    "    regular expression pattern (supports unicode).\n",
    "    \"\"\"\n",
    "    pattern = Param(\n",
    "        Params._dummy(), \"pattern\", \n",
    "        \"a regular expression for allocating tokens\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol:str=None, outputCol:str=None, pattern:str=\"[\\w]{2,}\"):\n",
    "        super(CustomRegexTokenizer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, data):  \n",
    "        tokenize = F.udf(\n",
    "            returnType=ArrayType(StringType()),\n",
    "            f=lambda text, pattern: re.findall(re.compile(pattern, re.U), text.lower())\n",
    "        )\n",
    "        \n",
    "        return data.withColumn(\n",
    "            self.getOutputCol(), \n",
    "            tokenize(\n",
    "                self.getInputCol(), \n",
    "                F.lit(self.getOrDefault(\"pattern\"))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## My courses to make recommendations\n",
    "given_courses = [\n",
    "    [23126, u'en'], [21617, u'en'], [16627, u'es'], [11556, u'es'], [16704, u'ru'], [13702, u'ru']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithm√©tique: en ...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses = ss.read.json(\"/labs/slaba02/DO_record_per_line.json\")\n",
    "courses.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_languages = {\n",
    "    \"ru\": \"russian\",\n",
    "    \"en\": \"english\",\n",
    "    \"es\": \"spanish\"\n",
    "}\n",
    "\n",
    "tokenizer = CustomRegexTokenizer(inputCol=\"desc\", outputCol=\"tokens\", pattern=u\"[\\w\\d]{2,}\")\n",
    "\n",
    "pipelines = {}\n",
    "for key_lang, language in map_languages.items():\n",
    "    \n",
    "    removerStopWords = StopWordsRemover(\n",
    "        inputCol=tokenizer.getOutputCol(), \n",
    "        outputCol=\"clear_tokens\", \n",
    "        stopWords=StopWordsRemover.loadDefaultStopWords(language)\n",
    "    )\n",
    "    \n",
    "    hasher = HashingTF(numFeatures=10_000, binary=False, inputCol=removerStopWords.getOutputCol(), outputCol=\"tf_embedding\")\n",
    "    tfidf  = IDF(inputCol=hasher.getOutputCol(), outputCol=\"embedding\")\n",
    "    \n",
    "    pipelines[key_lang] = Pipeline(stages=[tokenizer, removerStopWords, hasher, tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 141 ms, sys: 54.9 ms, total: 195 ms\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "courses_with_candidates = {}\n",
    "\n",
    "for lang in map_languages.keys():\n",
    "    courses_by_lang = courses.filter(f\"lang = '{lang}'\")\n",
    "    preprocessed_courses = pipelines[lang].fit(courses_by_lang).transform(courses_by_lang)\n",
    "    \n",
    "    candidates_courses = preprocessed_courses\n",
    "    for col in candidates_courses.columns:\n",
    "        candidates_courses = candidates_courses.withColumnRenamed(col, \"candidate_\" + col)\n",
    "    \n",
    "    courses_with_candidates[lang] = preprocessed_courses\\\n",
    "                                    .join(\n",
    "                                        other=candidates_courses,\n",
    "                                        how=\"left\",\n",
    "                                        on=(\n",
    "                                            (F.col(\"lang\") == F.col(\"candidate_lang\"))\n",
    "                                            & (F.col(\"id\") != F.col(\"candidate_id\"))\n",
    "                                        )\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=FloatType())\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    similarity = float(vector1.dot(vector2) / (vector1.norm(2) * vector2.norm(2)))\n",
    "    return similarity if pd.notna(similarity) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 1.3 ms, total: 16 ms\n",
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = {}\n",
    "for (course_id, lang_course) in given_courses:\n",
    "    relevant_courses = courses_with_candidates[lang_course]\\\n",
    "                       .where(f\"id = {course_id}\")\\\n",
    "                       .withColumn(\"similarity\", cosine_similarity(\"embedding\", \"candidate_embedding\"))\\\n",
    "                       .sort(F.col(\"similarity\").desc(), F.col(\"candidate_name\").asc(), F.col(\"candidate_id\").asc())\n",
    "    \n",
    "    result[course_id] = relevant_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:59<00:00,  9.87s/it]\n"
     ]
    }
   ],
   "source": [
    "num_recommendations = 10\n",
    "result_for_saving = {}\n",
    "\n",
    "for course_id, recommendations in tqdm(result.items()):\n",
    "    candidate_ids = recommendations.limit(num_recommendations).select(\"candidate_id\").collect()\n",
    "    result_for_saving[f\"{course_id}\"] = [row[\"candidate_id\"] for row in candidate_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab02.json\", \"w\") as f:\n",
    "    json.dump(result_for_saving, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.catalog.clearCache()\n",
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
