{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>artem.spitsin_lab04</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb01a14af60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/anaconda/envs/bd9/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]     = \"/usr/hdp/current/spark2-client\"\n",
    "\n",
    "spark_home = os.environ.get(\"SPARK_HOME\", None)\n",
    "if not spark_home:\n",
    "    raise ValueError(\"SPARK_HOME environment variable is not set\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))\n",
    "\n",
    "import pyspark\n",
    "from pyspark import keyword_only\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import (\n",
    "    Transformer, \n",
    "    Pipeline, \n",
    "    PipelineModel\n",
    ")\n",
    "\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler,\n",
    "    StringIndexer,\n",
    "    IndexToString,\n",
    "    CountVectorizer\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "conf = SparkConf()\\\n",
    "       .setAppName(\"artem.spitsin_lab04\")\\\n",
    "       .set(\"spark.driver.cores\", \"2\")\\\n",
    "       .set(\"spark.driver.memory\", \"1g\")\\\n",
    "       .set(\"spark.executor.instances\", \"5\")\\\n",
    "       .set(\"spark.executor.cores\", \"6\")\\\n",
    "       .set(\"spark.executor.memory\", \"4g\")\\\n",
    "       .set(\"spark.executor.memoryOverhead\", \"2g\")\\\n",
    "       .set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "ss = SparkSession\\\n",
    "     .builder\\\n",
    "     .appName(\"artem.spitsin_lab04\")\\\n",
    "     .config(conf=conf)\\\n",
    "     .getOrCreate()\n",
    "\n",
    "sc = ss.sparkContext\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = \"/labs/slaba04/gender_age_dataset.txt\"\n",
    "path_model_pipeline = \"hdfs:///user/artem.spitsin/lab04/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema convert to string for the ability to save a custom transformer \n",
    "json_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"timestamp\", LongType(), True),\n",
    "        StructField(\"url\", StringType(), True)\n",
    "    ])\n",
    ").simpleString()\n",
    "\n",
    "schema_train_data = StructType([\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"uid\", StringType(), True),\n",
    "    StructField(\"user_json\", StringType(), True)\n",
    "])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HasBaseCols(Param):\n",
    "    baseCols = Param(\n",
    "        Params._dummy(), \"baseCols\", \n",
    "        \"list of fields to be saved after parsing\", \n",
    "        typeConverter=TypeConverters.toListString\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasBaseCols, self).__init__()\n",
    "        \n",
    "    def setBaseCols(self, baseCols):\n",
    "        if isinstance(baseCols, list):\n",
    "            self._set(baseCols=baseCols)\n",
    "        elif isinstance(baseCols, str):\n",
    "            self._set(baseCols=[baseCols])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid data type for 'baseCols', a list or string was expected.\")\n",
    "            \n",
    "    def getBaseCols(self):\n",
    "        return self.getOrDefault(\"baseCols\")\n",
    "\n",
    "class ParserJson(Transformer, HasBaseCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Converts nested data to a tabular view.\n",
    "    \"\"\"\n",
    "    jsonSchema = Param(Params._dummy(), \"jsonSchema\", \"json schema that is parsed\", typeConverter=TypeConverters.toString)\n",
    "    jsonCol = Param(\n",
    "        Params._dummy(), \"jsonCol\", \"name of the field with the json structure\", typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, baseCols:list=\"uid\", jsonCol:str=\"json_value\", jsonSchema:str=None):\n",
    "        super(ParserJson, self).__init__()\n",
    "        self._set(**self._input_kwargs)\n",
    "\n",
    "    def _transform(self, data): \n",
    "        jsonCol, jsonSchema = self.getOrDefault(\"jsonCol\"), self.getOrDefault(\"jsonSchema\")\n",
    "\n",
    "        exploded_data = data\\\n",
    "                        .withColumn(jsonCol, F.from_json(jsonCol, schema=jsonSchema))\\\n",
    "                        .select(*self.getBaseCols(), F.explode_outer(jsonCol).alias(\"logs\"))\n",
    "        \n",
    "        flatten_structs = [f\"{field.name}.*\" for field in exploded_data.schema.fields if type(field.dataType) == StructType] \n",
    "        \n",
    "        return exploded_data.select(*self.getBaseCols(), *flatten_structs)\n",
    "\n",
    "class DataLogPreparer(Transformer, HasBaseCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Prepares data before generating features.\n",
    "    \"\"\"\n",
    "    urlCol = Param(\n",
    "        Params._dummy(), \"urlCol\", \n",
    "        \"name of the field with the url that the user visited\", \n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    logDateCol = Param(\n",
    "        Params._dummy(), \"logDateCol\", \n",
    "        \"name of the field with the value of the time when the user visited a particular url\", \n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, baseCols:list=\"uid\", urlCol:str=\"url\", logDateCol:str=\"timestamp\"):\n",
    "        super(DataLogPreparer, self).__init__()\n",
    "        self._set(**self._input_kwargs)\n",
    "        \n",
    "    def __create_time_features(self, data):\n",
    "        return data\\\n",
    "               .withColumn(\"log_dt\", F.from_unixtime(F.col(self.getOrDefault(\"logDateCol\")) / 1000))\\\n",
    "               .withColumn(\"day_of_week\", F.dayofweek(\"log_dt\"))\\\n",
    "               .withColumn(\"hour\", F.hour(\"log_dt\"))\\\n",
    "               .drop(\"log_dt\")\n",
    "    \n",
    "    def __get_clean_url(self, data):\n",
    "        url_col = self.getOrDefault(\"urlCol\")\n",
    "        return data\\\n",
    "               .withColumn(f\"clean_{url_col}\", F.split(url_col, \"/\")[2])\\\n",
    "               .withColumn(\"host\", F.regexp_replace(f\"clean_{url_col}\", \"www.\", \"\"))\\\n",
    "               .drop(f\"clean_{url_col}\")\n",
    "    \n",
    "    def _transform(self, data):\n",
    "        prepared_data = self.__create_time_features(data)\n",
    "        \n",
    "        return self.__get_clean_url(prepared_data)\\\n",
    "               .groupby(*self.getBaseCols())\\\n",
    "               .agg(\n",
    "                   F.collect_list(\"host\").alias(\"logged_hosts\"),\n",
    "                   F.mean(\"day_of_week\").alias(\"mean_day_of_week\"),\n",
    "                   F.mean(\"hour\").alias(\"mean_hour\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparation training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36138, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_targets = \"gender != '-' and age != '-'\"\n",
    "\n",
    "train_data = ss.read.options(delimiter=\"\\t\", header=True)\\\n",
    "               .schema(schema_train_data).csv(path_train_data)\\\n",
    "               .where(filter_targets)\\\n",
    "               .withColumn(\"visits\", F.json_tuple(\"user_json\", \"visits\"))\n",
    "\n",
    "train_data.count(), len(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stages for preparation data\n",
    "base_cols = [\"uid\", \"gender\", \"age\"]\n",
    "parser_json = ParserJson(baseCols=base_cols, jsonCol=\"visits\", jsonSchema=json_schema)\n",
    "preparer_fields = DataLogPreparer(baseCols=base_cols, urlCol=\"url\", logDateCol=\"timestamp\")\n",
    "\n",
    "# Stages for feature engineering\n",
    "count_vectorizer = CountVectorizer(inputCol=\"logged_hosts\", outputCol=\"hosts_features\")\n",
    "vector_assembler = VectorAssembler(inputCols=[\"hosts_features\", \"mean_day_of_week\", \"mean_hour\"], outputCol=\"features\")\n",
    "\n",
    "# Stages for modeling\n",
    "target_encoders, target_decoders, models = {}, {}, {}\n",
    "for type_target in [\"gender\", \"age\"]:\n",
    "    \n",
    "    target_encoders[type_target] = StringIndexer(inputCol=type_target, outputCol=f\"{type_target}_label\")\\\n",
    "                                   .fit(train_data)\n",
    "    \n",
    "    models[type_target] = RandomForestClassifier(\n",
    "        labelCol=target_encoders[type_target].getOutputCol(),\n",
    "        probabilityCol=f\"probability_{type_target}\", \n",
    "        rawPredictionCol=f\"rawPrediction_{type_target}\",\n",
    "        predictionCol=f\"prediction_{type_target}\",\n",
    "        featuresCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    target_decoders[type_target] = IndexToString(\n",
    "        inputCol=f\"prediction_{type_target}\", outputCol=f\"prediction_{type_target}_decode\",\n",
    "        labels=target_encoders[type_target].labels\n",
    "    )\n",
    "\n",
    "# Creating of pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    parser_json,\n",
    "    preparer_fields,\n",
    "    count_vectorizer,\n",
    "    vector_assembler,\n",
    "    *target_encoders.values(),\n",
    "    *models.values(),\n",
    "    *target_decoders.values()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94.6 ms, sys: 23.5 ms, total: 118 ms\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Saving fitted pipeline\n",
    "pipeline.fit(train_data).write().overwrite().save(path_model_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fb01a142d68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_artem.spitsin\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": \"False\"\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"artem.spitsin\"\n",
    "}\n",
    "\n",
    "kafka_sdf = ss.readStream.format(\"kafka\").options(**read_kafka_params).load()\\\n",
    "            .select(\n",
    "                F.json_tuple(F.col(\"value\").cast(\"string\"), \"uid\", \"visits\")\n",
    "            )\\\n",
    "            .select(F.col(\"c0\").alias(\"uid\"), F.col(\"c1\").alias(\"visits\"))\n",
    "\n",
    "# Get predictions\n",
    "fitted_pipeline = PipelineModel.load(path_model_pipeline)\n",
    "for ind_stage in range(2):\n",
    "    fitted_pipeline.stages[ind_stage].setBaseCols([\"uid\"])\n",
    "\n",
    "fitted_pipeline\\\n",
    ".transform(kafka_sdf)\\\n",
    ".select(\n",
    "    F.to_json(\n",
    "        F.struct(\n",
    "            \"uid\",\n",
    "            F.col(\"prediction_gender_decode\").alias(\"gender\"),\n",
    "            F.col(\"prediction_age_decode\").alias(\"age\")\n",
    "        )\n",
    "    ).alias(\"value\")\n",
    ")\\\n",
    ".writeStream\\\n",
    ".format(\"kafka\")\\\n",
    ".options(**write_kafka_params)\\\n",
    ".outputMode(\"complete\")\\\n",
    ".option(\"checkpointLocation\", \"/user/artem.spitsin/laba04/checkpoint\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.catalog.clearCache()\n",
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
