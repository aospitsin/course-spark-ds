{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@21273bf5\n",
       "ss = org.apache.spark.sql.SparkSession@3c320d04\n",
       "sc = org.apache.spark.SparkContext@31c2aa74\n",
       "sqlContext = org.apache.spark.sql.SQLContext@59cb8...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@59cb8..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import sys.process._ // for using console commands\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{SQLContext, DataFrame, Row}\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.linalg.{Vector}\n",
    "import org.apache.spark.ml.feature.{VectorAssembler}\n",
    "import org.apache.spark.ml.classification.GBTClassifier\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "val conf = new SparkConf()\n",
    "               .setAppName(\"artem.spitsin_lab05\")\n",
    "               .set(\"spark.driver.cores\", \"2\")\n",
    "               .set(\"spark.driver.memory\", \"1g\")\n",
    "               .set(\"spark.executor.instances\", \"5\")\n",
    "               .set(\"spark.executor.cores\", \"6\")\n",
    "               .set(\"spark.executor.memory\", \"4g\")\n",
    "               .set(\"spark.executor.memoryOverhead\", \"2g\")\n",
    "               .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "               \n",
    "val ss = SparkSession\n",
    "         .builder()\n",
    "         .appName(\"artem.spitsin_lab05\")\n",
    "         .config(conf=conf)\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "\n",
    "var sc = ss.sparkContext\n",
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vector2array = UserDefinedFunction(<function1>,ArrayType(DoubleType,false),Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "load_data: (path: String)org.apache.spark.sql.DataFrame\n",
       "drop_str_cols: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,ArrayType(DoubleType,false),Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(path: String) = {\n",
    "    spark.read\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .format(\"csv\").load(path)\n",
    "    .drop(\"_c0\")\n",
    "}\n",
    "\n",
    "def drop_str_cols(data: org.apache.spark.sql.DataFrame) = {\n",
    "    var drop_cols = ArrayBuffer[String]()\n",
    "    for (name_dtype <- data.dtypes if (name_dtype._2 == \"StringType\")) drop_cols += name_dtype._1\n",
    "    \n",
    "    data.drop(drop_cols:_*)\n",
    "}\n",
    "\n",
    "val vector2array = udf((v: Vector) => v.toArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_TARGET = TARGET\n",
       "SERVICE_COLS = List(ID, TARGET)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(ID, TARGET)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NAME_TARGET: String = \"TARGET\"\n",
    "val SERVICE_COLS: List[String] = List(\"ID\", NAME_TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train_data = [ID: int, CR_PROD_CNT_IL: int ... 101 more fields]\n",
       "test_data = [ID: int, CR_PROD_CNT_IL: int ... 100 more fields]\n",
       "train_data = [ID: int, CR_PROD_CNT_IL: int ... 101 more fields]\n",
       "test_data = [ID: int, CR_PROD_CNT_IL: int ... 100 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, CR_PROD_CNT_IL: int ... 100 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var train_data = load_data(path=\"/labs/slaba05/lab05_train.csv\")\n",
    "var test_data = load_data(path=\"/labs/slaba05/lab05_test.csv\")\n",
    "\n",
    "train_data = drop_str_cols(train_data)\n",
    "test_data = drop_str_cols(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data = [ID: int, CR_PROD_CNT_IL: int ... 101 more fields]\n",
       "train_data = [ID: int, CR_PROD_CNT_IL: int ... 101 more fields]\n",
       "test_data = [ID: int, CR_PROD_CNT_IL: int ... 100 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, CR_PROD_CNT_IL: int ... 100 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.where(\"%s is not null\".format(NAME_TARGET))\n",
    "train_data = train_data.na.fill(-1e5)\n",
    "\n",
    "test_data = test_data.na.fill(-1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection features and preparation for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "select_features = ArrayBuffer(CR_PROD_CNT_IL, AMOUNT_RUB_CLO_PRC, PRC_ACCEPTS_A_EMAIL_LINK, APP_REGISTR_RGN_CODE, PRC_ACCEPTS_A_POS, PRC_ACCEPTS_A_TK, TURNOVER_DYNAMIC_IL_1M, CNT_TRAN_AUT_TENDENCY1M, SUM_TRAN_AUT_TENDENCY1M, AMOUNT_RUB_SUP_PRC, PRC_ACCEPTS_A_AMOBILE, SUM_TRAN_AUT_TENDENCY3M, PRC_ACCEPTS_TK, PRC_ACCEPTS_A_MTP, REST_DYNAMIC_FDEP_1M, CNT_TRAN_AUT_TENDENCY3M, CNT_ACCEPTS_TK, REST_DYNAMIC_SAVE_3M, CR_PROD_CNT_VCU, REST_AVG_CUR, CNT_TRAN_MED_TENDENCY1M, AMOUNT_RUB_NAS_PRC, TRANS_COUNT_SUP_PRC, CNT_TRAN_CLO_TENDENCY1M, SUM_TRAN_MED_TENDENCY1M, PRC_ACCEPTS_A_ATM, PRC_ACCEPTS_MTP, TRANS_COUNT_NAS_PRC, CNT_ACCEPTS_MTP, CR_PROD_CNT_TOVR, CR_PROD_CNT_PIL, SUM_TRAN_CLO_TENDENCY1M, TURNOVER_CC, TRANS_COUNT_ATM_PRC, AMOUNT_RUB_ATM_PRC, TUR...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ArrayBuffer(CR_PROD_CNT_IL, AMOUNT_RUB_CLO_PRC, PRC_ACCEPTS_A_EMAIL_LINK, APP_REGISTR_RGN_CODE, PRC_ACCEPTS_A_POS, PRC_ACCEPTS_A_TK, TURNOVER_DYNAMIC_IL_1M, CNT_TRAN_AUT_TENDENCY1M, SUM_TRAN_AUT_TENDENCY1M, AMOUNT_RUB_SUP_PRC, PRC_ACCEPTS_A_AMOBILE, SUM_TRAN_AUT_TENDENCY3M, PRC_ACCEPTS_TK, PRC_ACCEPTS_A_MTP, REST_DYNAMIC_FDEP_1M, CNT_TRAN_AUT_TENDENCY3M, CNT_ACCEPTS_TK, REST_DYNAMIC_SAVE_3M, CR_PROD_CNT_VCU, REST_AVG_CUR, CNT_TRAN_MED_TENDENCY1M, AMOUNT_RUB_NAS_PRC, TRANS_COUNT_SUP_PRC, CNT_TRAN_CLO_TENDENCY1M, SUM_TRAN_MED_TENDENCY1M, PRC_ACCEPTS_A_ATM, PRC_ACCEPTS_MTP, TRANS_COUNT_NAS_PRC, CNT_ACCEPTS_MTP, CR_PROD_CNT_TOVR, CR_PROD_CNT_PIL, SUM_TRAN_CLO_TENDENCY1M, TURNOVER_CC, TRANS_COUNT_ATM_PRC, AMOUNT_RUB_ATM_PRC, TUR..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var select_features = ArrayBuffer[String]()\n",
    "for(col <- train_data.columns if (SERVICE_COLS.indexOf(col) == -1)) select_features += col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320763\n",
      "44399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembler_values = vecAssembler_eb3daa771a60\n",
       "train_data = [ID: int, CR_PROD_CNT_IL: int ... 102 more fields]\n",
       "test_data = [ID: int, CR_PROD_CNT_IL: int ... 101 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, CR_PROD_CNT_IL: int ... 101 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler_values = new VectorAssembler()\n",
    "                           .setInputCols(select_features.toArray)\n",
    "                           .setOutputCol(\"features\")\n",
    "\n",
    "train_data = assembler_values.transform(train_data).cache()\n",
    "test_data = assembler_values.transform(test_data).cache()\n",
    "\n",
    "println(train_data.count()); println(test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "booster = GBTClassificationModel (uid=gbtc_f7f524e1baad) with 70 trees\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GBTClassificationModel (uid=gbtc_f7f524e1baad) with 70 trees"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val booster = new GBTClassifier()\n",
    "                  .setFeaturesCol(\"features\")\n",
    "                  .setLabelCol(NAME_TARGET)\n",
    "                  .setMaxDepth(5)\n",
    "                  .setMaxIter(70)\n",
    "                  .fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_predictions = [ID: int, CR_PROD_CNT_IL: int ... 105 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, CR_PROD_CNT_IL: int ... 105 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_predictions = booster.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = binEval_3e8c51102242\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.845954265858909"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "                    .setLabelCol(NAME_TARGET)\n",
    "                    .setRawPredictionCol(\"probability\")\n",
    "                    .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "evaluator.evaluate(train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_predictions = [id: int, target: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res47: String = \"\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, target: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_predictions = booster.transform(test_data)\n",
    "                       .select(\n",
    "                           col(\"ID\").alias(\"id\"),\n",
    "                           vector2array(col(\"probability\")).getItem(1).alias(\"target\")\n",
    "                       )\n",
    "\n",
    "test_predictions.coalesce(1).write.option(\"header\", \"true\").option(\"sep\", \"\\t\").mode(\"overwrite\").csv(\"lab05.csv\")\n",
    "\"\"\"hdfs dfs -get lab05.csv\"\"\".!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.catalog.clearCache()\n",
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
