{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>artem.spitsin_lab01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff0b515f208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/anaconda/envs/bd9/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]     = \"/usr/hdp/current/spark2-client\"\n",
    "\n",
    "spark_home = os.environ.get(\"SPARK_HOME\", None)\n",
    "if not spark_home:\n",
    "    raise ValueError(\"SPARK_HOME environment variable is not set\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import Row\n",
    "\n",
    "conf = SparkConf()\\\n",
    "       .setAppName(\"artem.spitsin_lab01\")\\\n",
    "       .set(\"spark.executor.instances\", \"2\")\n",
    "\n",
    "ss = SparkSession\\\n",
    "     .builder\\\n",
    "     .appName(\"artem.spitsin_lab01\")\\\n",
    "     .config(conf=conf)\\\n",
    "     .getOrCreate()\n",
    "\n",
    "sc = ss.sparkContext\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp2date(timestamp:str):\n",
    "    return datetime.fromtimestamp(int(timestamp)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def count_rating_films(data:pyspark.rdd.PipelinedRDD, value_rating:int):\n",
    "    return data.filter(lambda row: row.rating == value_rating).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id=242, rating=3, time='1997-12-04 18:55:49', user_id=196),\n",
       " Row(item_id=302, rating=3, time='1998-04-04 23:22:22', user_id=186),\n",
       " Row(item_id=377, rating=1, time='1997-11-07 10:18:36', user_id=22),\n",
       " Row(item_id=51, rating=2, time='1997-11-27 08:02:03', user_id=244),\n",
       " Row(item_id=346, rating=1, time='1998-02-02 08:33:16', user_id=166)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"/labs/laba01/ml-100k/u.data\").map(lambda row: row.split(\"\\t\"))\n",
    "\n",
    "data = data.map(\n",
    "    lambda row: Row(\n",
    "        user_id = int(row[0]),\n",
    "        item_id = int(row[1]),\n",
    "        rating  = int(row[2]),\n",
    "        time    = timestamp2date(row[3])\n",
    "    )\n",
    ")\n",
    "\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hist_film': [6, 20, 43, 123, 103],\n",
       " 'hist_all': [6110, 11370, 27145, 34174, 21201]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_my_film = data.filter(lambda row: row.item_id == 96)\n",
    "\n",
    "result_analysis = {\n",
    "    \"hist_film\": [],\n",
    "    \"hist_all\" : [] \n",
    "}\n",
    "\n",
    "for value_rating in range(1, 6):\n",
    "    result_analysis[\"hist_film\"].append(count_rating_films(data_my_film, value_rating))\n",
    "    result_analysis[\"hist_all\"].append(count_rating_films(data, value_rating))\n",
    "    \n",
    "result_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(\n",
    "    result_analysis,\n",
    "    open(\"lab01.json\", \"w\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.catalog.clearCache()\n",
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
