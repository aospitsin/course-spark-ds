{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>artem.spitsin_lab03</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd10814eb8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/anaconda/envs/bd9/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]     = \"/usr/hdp/current/spark2-client\"\n",
    "\n",
    "spark_home = os.environ.get(\"SPARK_HOME\", None)\n",
    "if not spark_home:\n",
    "    raise ValueError(\"SPARK_HOME environment variable is not set\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "conf = SparkConf()\\\n",
    "       .setAppName(\"artem.spitsin_lab03\")\\\n",
    "       .set(\"spark.driver.cores\", \"2\")\\\n",
    "       .set(\"spark.driver.memory\", \"1g\")\\\n",
    "       .set(\"spark.executor.instances\", \"4\")\\\n",
    "       .set(\"spark.executor.cores\", \"5\")\\\n",
    "       .set(\"spark.executor.memory\", \"2g\")\\\n",
    "       .set(\"spark.executor.memoryOverhead\", \"2g\")\\\n",
    "       .set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "ss = SparkSession\\\n",
    "     .builder\\\n",
    "     .appName(\"artem.spitsin_lab03\")\\\n",
    "     .config(conf=conf)\\\n",
    "     .getOrCreate()\n",
    "\n",
    "sc = ss.sparkContext\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agg_feature(data, groupby_cols:list, agg_expression):\n",
    "    return data.groupby(*groupby_cols).agg(agg_expression)\n",
    "\n",
    "def multiple_join_features(core_data, data_features:list, how:str=\"left\"):\n",
    "    for feature in data_features:\n",
    "        join_key = \"item_id\" if \"item_id\" in feature.columns else \"user_id\"\n",
    "        core_data = core_data.join(feature, on=join_key, how=how)\n",
    "        \n",
    "    return core_data\n",
    "\n",
    "def feature_importance(model, name_features:list) -> \"spark.DataFrame\":\n",
    "    feature_importances = model.featureImportances\n",
    "    data_importances = [\n",
    "        [name, float(feature_importances[ind])] for ind, name in enumerate(name_features)\n",
    "    ]\n",
    "    \n",
    "    return ss.createDataFrame(\n",
    "        data=data_importances, \n",
    "        schema=[\"feature\", \"importance\"]\n",
    "    ).sort(F.col(\"importance\").desc())\n",
    "\n",
    "@F.pandas_udf(returnType=DoubleType())\n",
    "def sigmoid(values):\n",
    "    return (1 / (1 + np.exp(-values))).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/labs/slaba03\"\n",
    "\n",
    "train_data = ss.read.csv(f\"{base_path}/laba03_train.csv\", header=True, sep=\",\", inferSchema=\"true\")\n",
    "test_data  = ss.read.csv(f\"{base_path}/laba03_test.csv\", header=True, sep=\",\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(3), train_data.printSchema();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "|   1654|  93629|    null|\n",
      "|   1654|   9980|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show(3), test_data.printSchema();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features based on laba03_train\n",
    "agg_expressions = [\n",
    "    (F.sum(\"purchase\"), \"num_purchases\"),\n",
    "    (F.sum(\"purchase\") / F.count(\"purchase\"), \"ratio_purchases\")\n",
    "]\n",
    "\n",
    "calc_agg_features = []\n",
    "for expression, name_feature in agg_expressions:\n",
    "    calc_agg_features += [\n",
    "        create_agg_feature(train_data, [\"user_id\"], expression.alias(f\"{name_feature}_by_user\")),\n",
    "        create_agg_feature(train_data, [\"item_id\"], expression.alias(f\"{name_feature}_by_item\"))\n",
    "    ]\n",
    "    \n",
    "# Join features\n",
    "train = multiple_join_features(train_data, calc_agg_features, how=\"left\")\n",
    "test  = multiple_join_features(test_data, calc_agg_features, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_features = [\n",
    "    \"num_purchases_by_user\", \"num_purchases_by_item\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=name_features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 0 ns, total: 14.5 ms\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_sample = assembler.transform(train).select(\"features\", \"purchase\").cache()\n",
    "test_sample  = assembler.transform(test).select(\"user_id\", \"item_id\", \"features\").cache()\n",
    "\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 ms, sys: 4.87 ms, total: 23.3 ms\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = dict(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"purchase\",\n",
    "    predictionCol=\"raw_predictions\",\n",
    "    numTrees=100,\n",
    "    maxDepth=5\n",
    ")\n",
    "\n",
    "# Training of regressor shows very good results :D\n",
    "model = RandomForestRegressor(**params).fit(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " feature    | num_purchases_by_user \n",
      " importance | 0.5037890713161708    \n",
      "-RECORD 1---------------------------\n",
      " feature    | num_purchases_by_item \n",
      " importance | 0.4962109286838291    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_importance(model, name_features).show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929630472361225"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions = model.transform(train_sample)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"purchase\", \n",
    "    rawPredictionCol=\"raw_predictions\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator.evaluate(train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 s, sys: 1.37 s, total: 28.6 s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = model.transform(test_sample)\n",
    "\n",
    "predictions\\\n",
    ".select(\n",
    "   \"user_id\", \n",
    "   \"item_id\",\n",
    "   sigmoid(\"raw_predictions\").alias(\"purchase\")\n",
    ")\\\n",
    ".orderBy([\"user_id\", \"item_id\"], ascending=True)\\\n",
    ".toPandas()\\\n",
    ".to_csv(\"lab03.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.catalog.clearCache()\n",
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
